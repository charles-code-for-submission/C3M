{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import gcsfs\n",
    "import urllib.request\n",
    "import subprocess\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from google.cloud import bigquery\n",
    "from collections import OrderedDict\n",
    "import pickle\n",
    "my_bucket = os.getenv(\"WORKSPACE_BUCKET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare input ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_patient_ids = pickle.load(open('ADRD_ids.pkl', 'rb'))\n",
    "all_patient_ids = all_patient_ids.person_id.tolist()\n",
    "print('Process a  set of patients', len(all_patient_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 30000\n",
    "batches = [all_patient_ids[i:i + batch_size] for i in range(0, len(all_patient_ids), batch_size)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dxdata processing, merge with condition, and get original code id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class dxData(object):\n",
    "    def __init__(self, save_path):\n",
    "        self.savepath = save_path # save if needed\n",
    "\n",
    "\n",
    "    def get_condition(self, ids, batches=True): \n",
    "        if batches is True:\n",
    "            data_batches = []\n",
    "            for batch in ids:\n",
    "\n",
    "                dx_encounter_controls_query = f\"\"\"\n",
    "                    SELECT DISTINCT co.person_id, co.condition_concept_id, co.condition_source_concept_id, \n",
    "                            co.condition_source_value, co.condition_start_date\n",
    "                    FROM {os.environ[\"WORKSPACE_CDR\"]}.condition_occurrence co\n",
    "                    LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c \n",
    "                        ON co.condition_concept_id = c.concept_id\n",
    "                    WHERE co.person_id IN ({', '.join(map(str, batch))})\n",
    "                        AND co.condition_concept_id != 0\n",
    "                        AND c.domain_id = 'Condition'\n",
    "                        AND  c.vocabulary_id = 'SNOMED'\n",
    "                    \"\"\"\n",
    "\n",
    "                batch_data = pd.read_gbq(dx_encounter_controls_query, \n",
    "                             dialect=\"standard\",\n",
    "                             use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                             progress_bar_type=\"tqdm_notebook\")\n",
    "\n",
    "                data_batches.append(batch_data)\n",
    "\n",
    "            data = pd.concat(data_batches, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            dx_encounter_controls_query = f\"\"\"\n",
    "                SELECT DISTINCT co.person_id, co.condition_concept_id, co.condition_source_concept_id, \n",
    "                        co.condition_source_value, co.condition_start_date\n",
    "                FROM {os.environ[\"WORKSPACE_CDR\"]}.condition_occurrence co\n",
    "                LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c \n",
    "                    ON co.condition_concept_id = c.concept_id\n",
    "                WHERE co.person_id IN ({', '.join(map(str, ids))})\n",
    "                    AND co.condition_concept_id != 0\n",
    "                    AND c.domain_id = 'Condition'\n",
    "                    AND  c.vocabulary_id = 'SNOMED'\n",
    "                \"\"\"\n",
    "            data = pd.read_gbq(dx_encounter_controls_query, \n",
    "                             dialect=\"standard\",\n",
    "                             use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                             progress_bar_type=\"tqdm_notebook\")\n",
    "            \n",
    "        if self.savepath is not None:\n",
    "            pickle.dump(data, open(f'{savepath1}.pkl', 'wb'))\n",
    "        return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "savepath1 = 'uf_large_r_diag' # by setting savepath, the extracted data will be automatically saved\n",
    "dxDataProcess = dxData(save_path=savepath1)\n",
    "\n",
    "dxdata = dxDataProcess.get_condition(batches)\n",
    "print('extract dx: ', dxdata.shape)\n",
    "\n",
    "\n",
    "dxdata = dxdata[~dxdata['condition_concept_id'].isin([0])]\n",
    "print('after checking zero concept id: ', dxdata.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Drug data definition, and get original source code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class drugData(object):\n",
    "\n",
    "    def __init__(self, savepath):\n",
    "        self.savepath = savepath\n",
    "\n",
    "        \n",
    "\n",
    "    def get_medication(self, ids, batches=True): \n",
    "        if batches is True:\n",
    "            data_batches = []\n",
    "            for batch in ids:\n",
    "\n",
    "                drug_unique_exposure_query = f\"\"\"\n",
    "                    SELECT DISTINCT de.person_id, de.drug_concept_id, de.drug_source_concept_id, \n",
    "                                    de.drug_source_value, de.drug_exposure_start_date\n",
    "                    FROM {os.environ[\"WORKSPACE_CDR\"]}.drug_exposure de\n",
    "                    LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c \n",
    "                        ON de.drug_concept_id = c.concept_id\n",
    "                    WHERE de.person_id IN ({', '.join(map(str, batch))})\n",
    "                      AND de.drug_concept_id != 0\n",
    "                      AND c.vocabulary_id IN ('RxNorm', 'RxNorm Extension')\n",
    "                      AND c.domain_id = 'Drug'\n",
    "                    \"\"\"\n",
    "\n",
    "                batch_data = pd.read_gbq(drug_unique_exposure_query, \n",
    "                             dialect=\"standard\",\n",
    "                             use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                             progress_bar_type=\"tqdm_notebook\")\n",
    "\n",
    "                data_batches.append(batch_data)\n",
    "\n",
    "            data = pd.concat(data_batches, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            drug_unique_exposure_query = f\"\"\"\n",
    "            SELECT DISTINCT de.person_id, de.drug_concept_id, de.drug_source_concept_id, \n",
    "                            de.drug_source_value, de.drug_exposure_start_date\n",
    "            FROM {os.environ[\"WORKSPACE_CDR\"]}.drug_exposure de\n",
    "            LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c \n",
    "                ON de.drug_concept_id = c.concept_id\n",
    "            WHERE de.person_id IN ({', '.join(map(str, ids))})\n",
    "              AND de.drug_concept_id != 0\n",
    "              AND c.vocabulary_id IN ('RxNorm', 'RxNorm Extension')\n",
    "              AND c.domain_id = 'Drug'\n",
    "            \"\"\"\n",
    "            data = pd.read_gbq(drug_unique_exposure_query, \n",
    "                             dialect=\"standard\",\n",
    "                             use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                             progress_bar_type=\"tqdm_notebook\")\n",
    "            \n",
    "        if self.savepath is not None:\n",
    "            pickle.dump(data, open(f'{savepath1}.pkl', 'wb'))\n",
    "        return data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conduct data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath1 = 'uf_large_r_drug'\n",
    "\n",
    "drugDataProcess = drugData(savepath1)\n",
    "ddrug = drugDataProcess.get_medication(batches, batches=True)\n",
    "print('get drug data', ddrug.shape)\n",
    "\n",
    "ddrug = ddrug[~ddrug['drug_concept_id'].isin([0])]\n",
    "\n",
    "print('filter drug data by 0 concept id', ddrug.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process and construct measurement/lab test features, get values and original code id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lb_data definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lbData(object):\n",
    "\n",
    "    \n",
    "    def __init__(self, savepath):\n",
    "\n",
    "        self.savepath = savepath\n",
    "    \n",
    "\n",
    "    def get_measurement(self, ids, batches=True): \n",
    "        if batches is True:\n",
    "            data_batches = []\n",
    "            for batch in ids:\n",
    "\n",
    "                query = f\"\"\"\n",
    "                    SELECT DISTINCT t.person_id, t.measurement_concept_id, t.measurement_date, t.measurement_datetime, t.value_as_number, t.unit_concept_id, t.unit_source_value, t.range_high, t.range_low, c.vocabulary_id, c.concept_code\n",
    "                    FROM {os.environ[\"WORKSPACE_CDR\"]}.measurement t\n",
    "                    LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c\n",
    "                        ON t.measurement_concept_id = c.concept_id\n",
    "                    WHERE t.person_id IN ({', '.join(map(str, batch))})\n",
    "                      AND t.measurement_concept_id  != 0\n",
    "                      AND c.vocabulary_id = 'LOINC'\n",
    "                      AND c.domain_id = 'Measurement'\n",
    "                      AND t.value_as_number IS NOT NULL\n",
    "                    \"\"\"\n",
    "\n",
    "                batch_data = pd.read_gbq(query, \n",
    "                         dialect=\"standard\",\n",
    "                         use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                         progress_bar_type=\"tqdm_notebook\")\n",
    "\n",
    "                data_batches.append(batch_data)\n",
    "\n",
    "            data = pd.concat(data_batches, ignore_index=True)\n",
    "\n",
    "        else:\n",
    "\n",
    "            query = f\"\"\"\n",
    "                SELECT DISTINCT t.person_id, t.measurement_concept_id, t.measurement_date, t.measurement_datetime, t.value_as_number, t.unit_concept_id, t.unit_source_value, t.range_high, t.range_low, c.vocabulary_id, c.concept_code\n",
    "                FROM {os.environ[\"WORKSPACE_CDR\"]}.measurement t\n",
    "                LEFT JOIN {os.environ[\"WORKSPACE_CDR\"]}.concept c\n",
    "                    ON t.measurement_concept_id = c.concept_id\n",
    "                WHERE t.person_id IN ({', '.join(map(str, ids))})\n",
    "                  AND t.measurement_concept_id  != 0\n",
    "                  AND c.vocabulary_id = 'LOINC'\n",
    "                  AND c.domain_id = 'Measurement'\n",
    "                  AND t.value_as_number IS NOT NULL\n",
    "                \"\"\"\n",
    "\n",
    "            batch_data = pd.read_gbq(query, \n",
    "                     dialect=\"standard\",\n",
    "                     use_bqstorage_api=(\"BIGQUERY_STORAGE_API_ENABLED\" in os.environ),\n",
    "                     progress_bar_type=\"tqdm_notebook\")\n",
    "            \n",
    "        if self.savepath is not None:\n",
    "            pickle.dump(data, open(f'{savepath1}.pkl', 'wb'))\n",
    "        return data\n",
    "\n",
    "\n",
    "    def get_filtered_lbdata(self, lb_data, keeplist, chunk_size=500000, save_bucket=False):\n",
    "        print('warning: deprecated')\n",
    "        allcolumns = lb_data.columns\n",
    "        filtered_df = pd.DataFrame(columns=allcolumns)\n",
    "        \n",
    "        for i in tqdm(range(0, len(lb_data), chunk_size)):\n",
    "            chunk = lb_data.iloc[i:i + chunk_size]\n",
    "            filtered_chunk = chunk[chunk['measurement_concept_id'].isin(keeplist)]\n",
    "#             filter_chunk_list.append(filtered_chunk)\n",
    "            filtered_df = pd.concat([filtered_df, filtered_chunk], axis=0)\n",
    "\n",
    "    \n",
    "        print('After measurement filtering', filtered_df.shape)\n",
    "        \n",
    "        return filtered_df\n",
    "\n",
    "# remove measurement concept id = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath1 = 'uf_large_r_lab'\n",
    "\n",
    "labDataProcess = lbData(savepath1)\n",
    "labdata = labDataProcess.get_measurement(batches, batches=True)\n",
    "print('get drug data', labdata.shape)\n",
    "\n",
    "labdata = labdata[~labdata['measurement_concept_id'].isin([0])]\n",
    "\n",
    "print('filter drug data by 0 concept id', labdata.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter by manual list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keeplist = [] # set your list of labs \n",
    "filtered_df = labDataProcess.get_filtered_lbdata(lb_data=labdata, keeplist=keeplist)\n",
    "filtered_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "634.983px",
    "width": "992.188px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
